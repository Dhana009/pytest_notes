{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fixtures\n",
    "# markers\n",
    "# parametrization\n",
    "# pytest.ini \n",
    "# assertions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Test Discovery and Naming Conventions\n",
    " pytest automatically discovers test files and test cases:\n",
    " Test file names should start with test_ or end with _test.py.\n",
    " Test functions should start with test_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Assertions\n",
    "Use Pythonâ€™s built-in assert keyword for validations.\n",
    "\n",
    "\n",
    "\n",
    "3. Fixtures\n",
    "Fixtures manage setup and teardown for tests.\n",
    "Defined using the @pytest.fixture decorator.\n",
    "Can be shared across multiple tests via conftest.py."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Parametrization\n",
    "Run a single test with multiple data inputs using @pytest.mark.parametrize.\n",
    "\n",
    "\n",
    "5. Markers\n",
    "Categorize or customize tests using markers.\n",
    "Built-in markers:\n",
    "@pytest.mark.skip: Skip a test.\n",
    "@pytest.mark.xfail: Mark a test as expected to fail.\n",
    "Custom markers must be registered in pytest.ini."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Plugins\n",
    "Extend pytest functionality using plugins.\n",
    "Common plugins:\n",
    "pytest-html: Generate HTML reports.\n",
    "pytest-xdist: Run tests in parallel.\n",
    "pytest-mock: Mock objects during testing.\n",
    "\n",
    "\n",
    "7. Command-Line Options\n",
    "Options to control test execution:\n",
    "-v: Verbose mode.\n",
    "-k \"expression\": Run tests matching a name or expression.\n",
    "-m \"marker\": Run tests with a specific marker.\n",
    "--html=report.html: Generate an HTML report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Assertions with Custom Messages\n",
    "Enhance debugging with meaningful failure messages.\n",
    "\n",
    "\n",
    "10. Test Reports and Logging\n",
    "Use --html for reports.\n",
    "Integrate logging for better test debugging.\n",
    "\n",
    "\n",
    "11. Handling Test Failures\n",
    "Rerun failed tests using pytest-rerunfailures.\n",
    "Capture screenshots or logs on failure with hooks like pytest_runtest_makereport."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Register Custom Markers\n",
    "Markers are used to categorize or add metadata to tests. You can define custom markers in pytest.ini to avoid warnings when using them.\n",
    "\n",
    "\n",
    "[pytest]\n",
    "markers =\n",
    "    smoke: Smoke tests\n",
    "    regression: Regression tests\n",
    "    api: Tests related to API\n",
    "\n",
    "\n",
    "@pytest.mark.smoke\n",
    "def test_smoke_test():\n",
    "    assert True\n",
    "\n",
    "\n",
    "2. Set Command-Line Options\n",
    "pytest.ini can set default command-line options that will be applied every time pytest is run. This eliminates the need to manually pass them in the terminal.\n",
    "\n",
    "[pytest]\n",
    "addopts = -v --maxfail=3 --disable-warnings\n",
    "\n",
    "\n",
    "3. Specify Test Paths\n",
    "Define the default directories or files where pytest should look for tests.\n",
    "\n",
    "[pytest]\n",
    "testpaths = tests\n",
    "\n",
    "\n",
    "5. Control Test Discovery\n",
    "Configure how pytest discovers tests, including the pattern for test file names and test function names.\n",
    "\n",
    "[pytest]\n",
    "python_files = test_*.py\n",
    "python_functions = test_*\n",
    "\n",
    "\n",
    "7. Configure Plugins\n",
    "Some pytest plugins can be configured through pytest.ini. For example, you can configure the pytest-html plugin to generate HTML reports.\n",
    "\n",
    "[pytest]\n",
    "html = report.html\n",
    "\n",
    "\n",
    "9. Configure Test Retry Behavior (with Plugins)\n",
    "If you have a plugin like pytest-rerunfailures, you can configure the number of retries for failed tests.\n",
    "\n",
    "[pytest]\n",
    "reruns = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fixtures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fixtures - setup and teardown\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture() #setup and teardown, setup is called before the test case and teardown is called after the test case\n",
    "def setup():\n",
    "    print('setup')\n",
    "    yield\n",
    "    print('teardown')\n",
    "\n",
    "\n",
    "def test_first(setup):\n",
    "    print('test_first')\n",
    "    assert 1==1\n",
    "\n",
    "def test_second(setup):\n",
    "    print('test_second')\n",
    "    assert 2==2\n",
    "\n",
    "\n",
    "#pytest -v -s #runs the test cases in verbose mode and shows the output of the test cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key Features of @pytest.fixture():\n",
    "Reusable Across Tests: Once defined, a fixture can be used in multiple test functions.\n",
    "\n",
    "Scoped Usage:\n",
    "\n",
    "scope='function': The default, recreates the fixture for each test.\n",
    "\n",
    "scope='module': The fixture is shared among all tests in the same module.\n",
    "\n",
    "scope='class': The fixture is shared among all methods in a class.\n",
    "\n",
    "scope='session': The fixture is shared across the entire test session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "\n",
    "@pytest.fixture(scope=\"class\")\n",
    "def setup():\n",
    "    print(\"I will be executing first\")\n",
    "    yield\n",
    "    print(\" I will execute last\")\n",
    "\n",
    "\n",
    "@pytest.fixture()\n",
    "def dataLoad():\n",
    "    print(\"user profile data is being created\")\n",
    "    return [\"Rahul\",\"Shetty\",\"rahulshettyacademy.com\"]\n",
    "\n",
    "\n",
    "@pytest.fixture(params=[(\"chrome\",\"Rahul\",\"shetty\"), (\"Firefox\",\"shetty\"), (\"IE\",\"SS\")])\n",
    "def crossBrowser(request):\n",
    "    return request.param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how to pass the fixtures to every test case without explicity mentioning it\n",
    "\n",
    "#conftest.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture()\n",
    "def setup():\n",
    "    print('setup')\n",
    "    yield\n",
    "    print('teardown')\n",
    "\n",
    "\n",
    "\n",
    "#test_file.py\n",
    "\n",
    "@pytest.mark.usefixtures('setup')\n",
    "class testexample:\n",
    "    def test_first(self):\n",
    "        print('test_first')\n",
    "        assert 1==1\n",
    "\n",
    "    def test_second(self):\n",
    "        print('test_second')\n",
    "        assert 2==2\n",
    "\n",
    "\n",
    "\n",
    "#now if i want to exceute the setup only once in the class at the beginning of the test cases\n",
    "\n",
    "#conftest.py\n",
    "\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture(scope='class')\n",
    "def setup():\n",
    "    print('setup')\n",
    "    yield\n",
    "    print('teardown')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test_file.py\n",
    "\n",
    "@pytest.mark.usefixtures('setup')\n",
    "class testexample:\n",
    "    def test_first(self):\n",
    "        print('test_first')\n",
    "        assert 1==1\n",
    "\n",
    "    def test_second(self):\n",
    "        print('test_second')\n",
    "        assert 2==2\n",
    "\n",
    "\n",
    "#what is scope is here ?\n",
    "        \n",
    "#scope is used to define the scope of the fixture, it can be function, class, module, session\n",
    "        \n",
    "#function - the fixture is called for every test case\n",
    "\n",
    "#class - the fixture is called only once for the class\n",
    "\n",
    "#module - the fixture is called only once for the module\n",
    "\n",
    "#session - the fixture is called only once for the session\n",
    "\n",
    "\n",
    "#how to run the test cases in parallel\n",
    "        \n",
    "#pytest -n 2 #runs the test cases in parallel with 2 threads\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytest html report\n",
    "\n",
    "#pip install pytest-html\n",
    "\n",
    "#pytest --html=report.html #generates the html report\n",
    "\n",
    "#pytest --html=report.html --self-contained-html #generates the html report with all the css and js files embedded in the html file\n",
    "\n",
    "#pytest --html=report.html --self-contained-html -v -s #generates the html report with all the css and js files embedded in the html file, in verbose mode and shows the output of the test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytest-html-reporter\n",
    "\n",
    "# pytest tests/ --html-report=./report --title='PYTEST REPORT'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logging\n",
    "\n",
    "import logging\n",
    "import inspect\n",
    "\n",
    "loggername = inspect.stack()[1][3] #gets the name of the function\n",
    "logger = logging.getLogger(__name__) # __name__ is the name of the module #logger is an object of the logging class\n",
    "\n",
    "\n",
    "filehandler = logging.FileHandler('logfile.log') #creates a file handler that writes the logging messages to the file\n",
    "\n",
    "formatter = logging.Formatter('%(asctime)s : %(levelname)s : %(name)s : %(message)s') #creates a formatter that formats the logging messages\n",
    "\n",
    "filehandler.setFormatter(formatter) #sets the formatter to the file handler\n",
    "\n",
    "logger.addHandler(filehandler) #adds a handler to the logger, it is used to handle the logging messages\n",
    "\n",
    "logger.setLevel(logging.DEBUG) #sets the logging level of the logger\n",
    "\n",
    "logger.debug('this is a debug message') #debug is the lowest level of logging, it is used for debugging purposes\n",
    "logger.info('this is an info message') #info is the second lowest level of logging, it is used for informational purposes\n",
    "logger.warning('this is a warning message') #warning is the third lowest level of logging, it is used for warning purposes\n",
    "logger.error('this is an error message') #error is the second highest level of logging, it is used for error purposes\n",
    "logger.critical('this is a critical message') #critical is the highest level of logging, it is used for critical purposes\n",
    "\n",
    "#logging levels\n",
    "\n",
    "#debug - lowest level of logging, it is used for debugging purposes\n",
    "#info - second lowest level of logging, it is used for informational purposes\n",
    "#warning - third lowest level of logging, it is used for warning purposes\n",
    "#error - second highest level of logging, it is used for error purposes\n",
    "#critical - highest level of logging, it is used for critical purposes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import inspect\n",
    "\n",
    "class BaseClass:\n",
    "\n",
    "    def getlogger(self):\n",
    "        # Get the name of the calling function\n",
    "        loggername = inspect.stack()[1][3]\n",
    "        \n",
    "        # Create a logger object with the name of the calling function\n",
    "        logger = logging.getLogger(loggername)\n",
    "        \n",
    "        # Create a file handler to write log messages to a file named 'logfile.log'\n",
    "        filehandler = logging.FileHandler('logfile.log')\n",
    "        \n",
    "        # Create a formatter to specify the format of log messages\n",
    "        formatter = logging.Formatter('%(asctime)s : %(levelname)s : %(name)s : %(message)s')\n",
    "        \n",
    "        # Set the formatter for the file handler\n",
    "        filehandler.setFormatter(formatter)\n",
    "        \n",
    "        # Add the file handler to the logger\n",
    "        logger.addHandler(filehandler) # Add handler to the logger object, so that it can write to the file, as well as to the console, if required, based on the logging level\n",
    "        \n",
    "        # Set the logging level to INFO\n",
    "        logger.setLevel(logging.INFO)\n",
    "        \n",
    "        # Return the logger object\n",
    "        return logger\n",
    "    \n",
    "\n",
    "\n",
    "import pytest\n",
    "from test_logging import BaseClass \n",
    "\n",
    "class TestEx(BaseClass):\n",
    "\n",
    "    def test_ets(self):\n",
    "        log = self.getlogger()\n",
    "        log.error(\"This is info message\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pytest -v #runs the test cases in verbose mode, it shows the output of the test cases\n",
    "\n",
    "#pytest -v -s #runs the test cases in verbose mode and shows console logs\n",
    "\n",
    "#pytest filename.py -v -s #runs the test cases in the filename.py, in verbose mode and shows the output of the test cases\n",
    "\n",
    "#pytest -k test_name #runs the test cases with the test_name, it is used to run the specific test cases\n",
    "\n",
    "# how to run selected test cases in pytest\n",
    "#pytest -k test_name #-k is a keyword argument that is used to run the specific test cases\n",
    "#example: \n",
    "#pytest -k login #-runs the test cases with the login keyword\n",
    "\n",
    "\n",
    "# how to run all the test cases in pytest\n",
    "#pytest\n",
    "\n",
    "# how to run the test cases in verbose mode\n",
    "#pytest -v\n",
    "\n",
    "# how to run the test cases in verbose mode and show the output\n",
    "#pytest -v -s\n",
    "\n",
    "# how to run the test cases in a specific file\n",
    "#pytest filename.py\n",
    "\n",
    "# how to run the test cases in a specific file in verbose mode\n",
    "#pytest filename.py -v\n",
    "\n",
    "#how to run specific test case in a specific file which is in a class\n",
    "#pytest filename.py::classname::test_name\n",
    "\n",
    "#how to run all the test cases in a specific file which is in a class\n",
    "#pytest filename.py::classname\n",
    "\n",
    "#marker is a keyword argument that is used to run the specific test cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.smoke #-m is a marker that is used to run the test cases with the specific tag\n",
    "@pytest.mark.skip()\n",
    "def test_first():\n",
    "    assert 1==1\n",
    "\n",
    "@pytest.mark.sanity() #sanity is a tag that is used to run the test cases with the sanity tag means the test cases that are important\n",
    "def test_second():\n",
    "    assert 2==2\n",
    "\n",
    "@pytest.mark.xfail(strict=True) #expected to fail, if the test case fails, it will not be considered as a failure\n",
    "def test_third():\n",
    "    assert 3==3\n",
    "\n",
    "\n",
    "\n",
    "#pytest -m smoke #runs the test cases with the smoke tag\n",
    "\n",
    "\n",
    "[pytest]\n",
    "markers =\n",
    "    run: Tests that should be executed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytest --maxfail=2  # stop after two failures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytest                   # Default summary showing only failed (f) and error (E) tests.\n",
    "\n",
    "pytest -rfs              # Show failed (f) and skipped (s) tests in the summary.\n",
    "\n",
    "pytest -rA               # Show all test results (passed, failed, skipped, xfailed, xpassed, etc.).\n",
    "\n",
    "pytest -rpP              # Show passed (p) tests and passed with output (P).\n",
    "\n",
    "pytest -ra               # Show all results except passed (p) and passed with output (P).\n",
    "\n",
    "pytest -rN               # Show nothing in the test summary (minimal output).\n",
    "\n",
    "pytest -rfsxX            # Show failed (f), skipped (s), xfailed (x), and xpassed (X).\n",
    "\n",
    "pytest -r                # Show default summary (fE), equivalent to not specifying -r.\n",
    "\n",
    "pytest -rf               # Show only failed (f) tests in the summary.\n",
    "\n",
    "pytest -rsx              # Show skipped (s) and xfailed (x) tests in the summary.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "s (skipped):\n",
    "The test was skipped intentionally, typically using a @pytest.mark.skip or pytest.skip() call.\n",
    "\n",
    "x (xfailed):\n",
    "The test was expected to fail (marked with @pytest.mark.xfail) and indeed failed.\n",
    "\n",
    "X (xpassed):\n",
    "The test was expected to fail (marked with @pytest.mark.xfail) but unexpectedly passed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.skip(reason=\"Feature not ready\")\n",
    "def test_skip():\n",
    "    assert False  # This test will be skipped\n",
    "\n",
    "@pytest.mark.xfail(reason=\"Known issue, fix pending\")\n",
    "def test_xfail():\n",
    "    assert False  # This test will be marked as xfail\n",
    "\n",
    "@pytest.mark.xfail(reason=\"Should fail but doesn't\", strict=True) # even if test case is passed will be treated as fail\n",
    "def test_xpass():\n",
    "    assert True  # This will result in xpass\n",
    "\n",
    "def test_error():\n",
    "    raise Exception(\"Something went wrong!\")  # This will be marked as error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False, \"This test is intentionally failed\"  # This will fail\n",
    "\n",
    "pytest.fail(\"This test is intentionally failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "\n",
    "@pytest.mark.smoke\n",
    "def test_smoke():\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.regression\n",
    "def test_regression():\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.slow\n",
    "def test_slow():\n",
    "    assert True\n",
    "\n",
    "@pytest.mark.fast\n",
    "def test_fast():\n",
    "    assert True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tests that contain 'smoke' in the name\n",
    "pytest -k \"smoke\"\n",
    "\n",
    "# Run tests that contain both 'fast' and 'slow'\n",
    "pytest -k \"fast and slow\"\n",
    "\n",
    "pytest -k \"not slow\" #Skip tests with the slow mark\n",
    "\n",
    "\n",
    "\n",
    "# Run all tests marked with 'smoke'\n",
    "pytest -m smoke\n",
    "\n",
    "# Run all tests marked with 'slow' or 'fast'\n",
    "pytest -m \"slow or fast\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
